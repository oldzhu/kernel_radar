# TAPRIO (sch_taprio) RCU stall / soft-lockup style failure

This document explains **why the repro’s TAPRIO schedule can starve RCU**, how it maps to the involved functions, and what the proposed mitigations do.

Scope / pointers:
- Repro bundle directory (ignored by git due to large artifacts): `kernel_radar/repro/f8850bc3986562f79619/`
- Serial automation runner (tracked): `tools/run_repro_serial.py` (use `--bundle-dir kernel_radar/repro/f8850bc3986562f79619`)

> TL;DR
> - The repro programs a `taprio` schedule with a **very small interval** (e.g. `0xff` = 255 **nanoseconds**).
> - The software (non-offloaded) TAPRIO implementation drives gate changes using an **absolute hrtimer** (`advance_sched`).
> - With tiny intervals, the timer expiry tends to be **already in the past** when the callback runs; the callback returns `HRTIMER_RESTART`, causing the hrtimer to fire again immediately.
> - This turns into a **timer “catch-up storm”** that monopolizes CPU time and **starves RCU**, leading to `rcu_preempt detected stalls`.

---

## 1) Symptom signature (what we observe)

When the repro runs under KVM, it reliably triggers:

- `rcu: INFO: rcu_preempt detected stalls ...`
- Backtrace / stack frequently includes the timer path and TAPRIO:
  - `advance_sched()` (TAPRIO hrtimer callback)
  - `__hrtimer_run_queues()` / `hrtimer_interrupt()`

In some reports the stall is *detected* while executing networking/bridge code paths (e.g. `br_handle_frame`). That’s often incidental: **RCU is complaining because a CPU is not reaching quiescent states**, and the CPU hog can be in a different context.

---

## 2) What the repro configures (the “input”)

The syzkaller program (see `repro.syz` in the repro bundle) uses netlink `RTM_NEWQDISC` to install a TAPRIO qdisc and provides schedule entries, including:

- `TCA_TAPRIO_SCHED_ENTRY_INTERVAL = 0xff`

In `sch_taprio.c` the interval is parsed as:

- `entry_policy[TCA_TAPRIO_SCHED_ENTRY_INTERVAL] = { .type = NLA_U32 }`
- `interval = nla_get_u32(tb[TCA_TAPRIO_SCHED_ENTRY_INTERVAL]);`

And it is treated as **nanoseconds** throughout the implementation.

---

## 3) “Normal” TAPRIO software flow (high-level)

### Flow graph: configuration → timer → dequeue

```mermaid
flowchart TD
  A[userspace netlink RTM_NEWQDISC taprio] --> B[taprio_change()]
  B --> C[parse_taprio_schedule()]
  C --> D[parse_sched_list()]
  D --> E[fill_sched_entry(): interval, gate_mask, cmd]

  B --> F[setup_first_end_time(): first end_time, budgets]
  B --> G[taprio_get_start_time(): choose next cycle start]
  B --> H[taprio_start_sched(): start advance_timer]

  H --> I[hrtimer fires]
  I --> J[advance_sched(): compute next entry/end_time]
  J --> K[hrtimer_set_expires(end_time) + HRTIMER_RESTART]
  J --> L[__netif_schedule(): run qdisc dequeue]

  L --> M[taprio_dequeue(): choose skb based on gates]
```

### Key point
In software mode (no full offload, no txtime-assist), TAPRIO’s “gate advance” is driven by a periodic/step timer callback: **`advance_sched()`**.

---

## 4) Where the pathology comes from

### 4.1 Validation currently allows very small intervals

In `fill_sched_entry()`:

- Minimum interval is checked against the **minimum Ethernet frame duration**:
  - `min_duration = length_to_duration(q, ETH_ZLEN)`
  - Reject if `interval < min_duration`

Where `length_to_duration()` uses a `picos_per_byte` derived from the link speed:

- `length_to_duration(q, len) = (len * picos_per_byte) / PSEC_PER_NSEC`

**Why this matters**: on fast links, `min_duration` can be extremely small.

Example intuition:
- At 10Gbps, 1 byte ~= 0.8ns.
- For 60 bytes, transmission time is about 48ns.
- So **255ns passes** easily.

### 4.2 `advance_sched()` uses absolute time and always restarts

`advance_sched()` computes a future `end_time` and then does:

- `hrtimer_set_expires(&q->advance_timer, end_time);`
- `return HRTIMER_RESTART;`

No “catch-up cap” exists here.

### 4.3 Why a “catch-up storm” occurs

With very small `interval` values:

1) The timer fires (or is scheduled to fire) at time `end_time`.
2) But by the time the CPU actually runs the callback, **`now > end_time`**.
3) The callback sets the timer expiry to an already-passed instant and returns `HRTIMER_RESTART`.
4) The hrtimer code treats this as “already expired” and re-invokes it promptly.

This quickly becomes a tight loop of:

- hrtimer interrupt/callback
- `advance_sched()` bookkeeping
- immediate re-fire

…and other kernel work (including RCU progress) gets starved.

### Failure loop graph

```mermaid
flowchart TD
  T[hrtimer interrupt] --> A[advance_sched()]
  A --> B[compute end_time = entry.end_time + next.interval]
  B --> C{end_time <= now?}
  C -- yes --> D[hrtimer_set_expires(end_time in past)]
  D --> E[return HRTIMER_RESTART]
  E --> T

  C -- no --> F[program next expiry normally]
  F --> E
```

This is the core RCA: **a user-controlled schedule interval can drive an effective hrtimer busy-loop**.

---

## 5) Why RCU complains (mechanism)

RCU needs CPUs to periodically report quiescent states / allow grace periods to complete.

A timer storm can prevent:
- scheduling
- normal softirq processing balance
- preemption points in the right places

So RCU prints stalls like:
- `rcu_preempt detected stalls`

Even if the stall is *reported* while executing `br_handle_frame` (or other networking paths), the CPU starvation source can be **the timer storm**.

---

## 6) Mitigation / fix options

There are multiple non-exclusive fixes. They can be layered.

### Option A: enforce a sane minimum interval for software scheduling

**Idea**: a software-driven gate schedule cannot be honored meaningfully at ~100ns granularity on general purpose CPUs; it becomes a DoS vector.

So add a stricter minimum interval when:
- not full offload
- not txtime-assist

Implementation idea:
- `min_interval = max(min_frame_duration, 1us)`
- reject with extack error if smaller

### Option B: run TAPRIO advance timer in soft hrtimer context

**Idea**: even with aggressive schedules, don’t execute the schedule-advance in hard IRQ context.

Implementation idea:
- start/init the timer using `HRTIMER_MODE_ABS_SOFT`

### Option C: explicit catch-up / skip-forward logic in `advance_sched()`

**Idea**: if the system is late, compute the correct entry for “now” and schedule the next boundary in the future, rather than rearming into the past repeatedly.

Design points:
- Ensure the next programmed `end_time` is **strictly > now**
- Bound work per callback

#### Concrete pseudo-code sketch

```c
// High-level idea, not drop-in code.
if (end_time <= now) {
    // Map 'now' into (base + k*cycle, base + (k+1)*cycle)
    // and pick the entry containing that offset.
    // Then compute the next boundary strictly in the future.
}
```

Catch-up control flow (conceptually):

```mermaid
flowchart TD
  T[advance_sched fired] --> N[now = taprio_get_time()]
  N --> S[compute next end_time normally]
  S --> C{end_time > now?}
  C -- yes --> ARM[arm timer at end_time]
  C -- no --> MAP[map now to cycle offset]
  MAP --> PICK[pick entry containing offset]
  PICK --> FUT[compute boundary strictly in future]
  FUT --> ARM
  ARM --> R[return HRTIMER_RESTART]
```

---

## 7) Validation ideas

- Confirm the original repro no longer triggers stalls.
- Confirm typical (microsecond+) schedules still work.
