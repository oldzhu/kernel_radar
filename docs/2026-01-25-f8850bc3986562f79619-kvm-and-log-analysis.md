# 2026-01-25 — f8850bc3986562f79619 — log analysis + KVM acceleration prep

## Goal
Make our local reproduction trustworthy by:
- Comparing local stall signatures to syzbot’s original CrashReport/CrashLog.
- Eliminating obvious “slow emulation / CPU starvation” artifacts (e.g. huge hrtimer delays) by enabling KVM acceleration.

## What we observed locally (run1/run4)
We extracted the highest-signal lines from local serial logs:

- `repro/f8850bc3986562f79619/qemu-serial-run1.log`
  - `hrtimer: interrupt took 87634924810 ns` (~87s)
  - `rcu: INFO: rcu_preempt detected stalls on CPUs/tasks:`
  - `rcu_preempt kthread starved for 10502 jiffies`
  - `Kernel panic - not syncing: softlockup: hung tasks`

- `repro/f8850bc3986562f79619/qemu-serial-run4.log`
  - `hrtimer: interrupt took 56068926360 ns` (~56s)
  - `rcu: INFO: rcu_preempt detected stalls on CPUs/tasks:`
  - `rcu_preempt kthread starved for 4893 jiffies`
  - `Kernel panic - not syncing: softlockup: hung tasks`

Interpretation:
- The *massive* hrtimer interrupt delays suggest the guest is not getting CPU time consistently.
- That can trigger apparent RCU starvation / watchdog softlockups even without the “real” bug.

## What syzbot reported (original crash report)
We inspected the syzbot CrashReport/CrashLog URLs saved in `repro/f8850bc3986562f79619/meta.txt`:

- CrashReport: `https://syzkaller.appspot.com/text?tag=CrashReport&x=16f4f642580000`
- CrashLog: `https://syzkaller.appspot.com/text?tag=CrashLog&x=17d98762580000`

Key point:
- The syzbot report’s call trace includes `br_handle_frame` in the bridge/netfilter forwarding path.

So the title `INFO: rcu detected stall in br_handle_frame (6)` is consistent with the original report.

## Root-cause hypothesis (today)
Right now we have **two overlapping problems**:

1) The *real* bug syzbot observed is on a bridge/netfilter path (involving `br_handle_frame`).
2) Our local environment likely adds a **CPU starvation / clock-jump artifact**, visible as enormous `hrtimer: interrupt took ... ns`, that can cause RCU/wd failures even if we are not in the exact same code path.

Therefore, enabling KVM is a priority so we can tell (1) from (2).

## Tooling improvement: optional KVM in runner
We updated the generated runner script to support optional KVM acceleration:

- In `repro/<extid>/run_qemu.sh`:
  - `ENABLE_KVM=1` enables `-enable-kvm` (requires `/dev/kvm`)
  - `CPU=host` optionally passes `-cpu host`

This change is committed/pushed:
- commit: `d5404a8` (tools: add optional KVM accel to QEMU runner)

Docs updated:
- `docs/tools-index.md` now mentions these env vars.

## Host status: /dev/kvm exists but user needs group
- `/dev/kvm` exists and is owned by group `kvm` (`crw-rw---- root:kvm`).
- Current user `oldzhu` is **not** in group `kvm` yet.

Next step (requires sudo):

```bash
sudo usermod -aG kvm oldzhu
# then log out/in (or restart WSL) so group membership refreshes
```

## Next
After KVM is enabled for the user:

1) Boot with acceleration:

```bash
cd repro/f8850bc3986562f79619
ENABLE_KVM=1 CPU=host SMP=2 MEM=4096 DAEMONIZE=1 ./run_qemu.sh
```

2) Re-run the ReproC.
3) Re-check serial log for:
- whether `hrtimer: interrupt took ...` giant delays disappear
- whether the stall call trace includes `br_handle_frame` (matching syzbot)
